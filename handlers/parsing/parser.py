# handlers/parsing/parser.py
from __future__ import annotations

import asyncio
import json
import re
import sys
import os
import traceback
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from aiogram import Router, F
from aiogram.types import CallbackQuery, InlineKeyboardButton, InlineKeyboardMarkup

# —Ç–≤–æ–∏ –∏–º–ø–æ—Ä—Ç—ã (–∫–∞–∫ –±—ã–ª–æ)
from telethon_manager import get_all_clients, resolve_entity, get_clients_for_user  # noqa
from handlers.auth_utils import auth_get

ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from handlers.normalizers.entry import run_build_parsed_goods, run_build_parsed_etalon
from handlers.parsing.context import set_parsing_data_dir, user_data_dir, DEFAULT_BASE_DIR

router = Router()

# =========================
# FILES
# =========================
MODULE_DIR = Path(__file__).parent.resolve()
DATA_DIR = (MODULE_DIR / "data").resolve()
DATA_DIR.mkdir(parents=True, exist_ok=True)
MESSAGES_FILE = DATA_DIR / "parsed_messages.json"


# =========================
# UI (collect menu + –∑–∞–ø—É—Å–∫)
# =========================
def collect_menu_keyboard() -> InlineKeyboardMarkup:
    return InlineKeyboardMarkup(
        inline_keyboard=[
            [InlineKeyboardButton(text="üìä –°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Ü–µ–Ω—ã", callback_data="collect_all")],
            [InlineKeyboardButton(text="‚¨ÖÔ∏è –ù–∞–∑–∞–¥", callback_data="main_menu")],
        ]
    )


@router.callback_query(F.data == "collect")
async def collect_menu(callback: CallbackQuery):
    u = await auth_get(callback.from_user.id)
    access = (u or {}).get("access") or {}
    if not u or not (u.get("role") == "admin" or access.get("products.collect")):
        await callback.answer("‚õîÔ∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞", show_alert=True)
        return
    if callback.message:
        await callback.message.answer("üè∑ –í—ã–±–µ—Ä–∏ —Ä–µ–∂–∏–º –ø–∞—Ä—Å–∏–Ω–≥–∞:", reply_markup=collect_menu_keyboard())
    try:
        await callback.answer()
    except Exception:
        pass
    if (u or {}).get("role") != "admin":
        set_parsing_data_dir(DEFAULT_BASE_DIR)


@router.callback_query(F.data == "show_unmatched")
async def show_unmatched(callback: CallbackQuery):
    if callback.message:
        await callback.message.answer(
            "‚ÑπÔ∏è Unmatched —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –ø–æ–∑–∂–µ (matcher/results). –ó–¥–µ—Å—å —Ç–æ–ª—å–∫–æ parsed_messages.json."
        )
    try:
        await callback.answer()
    except Exception:
        pass


def _reset_outputs() -> None:
    _reset_data_dir_files()



@router.callback_query(F.data == "clear_prices")
async def clear_prices(callback: CallbackQuery):
    _reset_outputs()
    if callback.message:
        await callback.message.answer("üóë parsed_messages.json –æ—á–∏—â–µ–Ω.", reply_markup=collect_menu_keyboard())
    try:
        await callback.answer()
    except Exception:
        pass


@router.callback_query(F.data == "collect_all")
async def collect_all(callback: CallbackQuery):
    u = await auth_get(callback.from_user.id)
    access = (u or {}).get("access") or {}
    if not u or not (u.get("role") == "admin" or access.get("products.collect")):
        await callback.answer("‚õîÔ∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞", show_alert=True)
        return
    if u.get("role") != "admin" and u.get("sources_mode") == "default":
        await callback.message.answer("‚õîÔ∏è –í —Ä–µ–∂–∏–º–µ '–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é' —Å–±–æ—Ä —Å–≤–æ–∏—Ö —Ü–µ–Ω –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
        return
    if u.get("role") == "admin":
        set_parsing_data_dir(DEFAULT_BASE_DIR)
    else:
        set_parsing_data_dir(user_data_dir(callback.from_user.id))
    _reset_outputs()

    # ‚úÖ –¥–µ—Ñ–æ–ª—Ç—ã, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–≤–∏—Ç—å UnboundLocalError –¥–∞–∂–µ –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ —É–ø–∞–¥—ë—Ç —Ä–∞–Ω—å—à–µ
    stats_sources: Dict[str, Any] = {}
    messages: List[Dict[str, Any]] = []
    errors_block = ""
    zeros_block = ""

    if callback.message:
        await callback.message.answer("üöÄ –ó–∞–ø—É—Å–∫–∞—é —Å–±–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π‚Ä¶")

    # --- collect stage ---
    if (u or {}).get("role") == "admin":
        messages, stats_sources = await collect_messages()
    else:
        sources_mode = (u or {}).get("sources_mode", "default")
        messages, stats_sources = await collect_messages(user_id=callback.from_user.id, sources_mode=sources_mode)

    # --- errors per source (collect stage) ---
    per = (stats_sources or {}).get("per_source") or []
    err_items = [x for x in per if isinstance(x, dict) and not x.get("ok")]
    zero_items = [
        x for x in per
        if isinstance(x, dict) and x.get("ok") and int(x.get("messages") or 0) == 0
    ]

    def _fmt_err(x: Dict[str, Any]) -> str:
        src = (x.get("source") or "Unknown").strip()
        err = (x.get("error") or "unknown error").strip()
        err = re.sub(r"\s+", " ", err)
        return f"‚Ä¢ <b>{src}</b> ‚Äî {err}"

    def _fmt_zero(x: Dict[str, Any]) -> str:
        src = (x.get("source") or "Unknown").strip()
        note = (x.get("skipped") or "no messages").strip()
        note = re.sub(r"\s+", " ", note)
        return f"‚Ä¢ <b>{src}</b> ‚Äî {note}"

    if err_items:
        MAX_ERR = 12
        shown = err_items[:MAX_ERR]
        tail = len(err_items) - len(shown)
        lines = "\n".join(_fmt_err(x) for x in shown)
        if tail > 0:
            lines += f"\n‚Ä¶–∏ –µ—â—ë <b>{tail}</b> –∏—Å—Ç–æ—á–Ω."
        errors_block = f"\n\n<b>–û—à–∏–±–∫–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:</b>\n{lines}"

    if zero_items:
        MAX_ZERO = 10
        shown = zero_items[:MAX_ZERO]
        tail = len(zero_items) - len(shown)
        lines = "\n".join(_fmt_zero(x) for x in shown)
        if tail > 0:
            lines += f"\n‚Ä¶–∏ –µ—â—ë <b>{tail}</b> –∏—Å—Ç–æ—á–Ω."
        zeros_block = f"\n\n<b>–ë–µ–∑ —Å–æ–æ–±—â–µ–Ω–∏–π:</b>\n{lines}"

    # ‚úÖ –î–û –ø–∞—Ä—Å–∏–Ω–≥–∞: –µ—Å–ª–∏ —à–∞–ø–∫–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º–æ–µ –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    messages = dedupe_messages_by_header_keep_latest(messages)

    parsed_messages = parse_messages(messages)
    _write_json(MESSAGES_FILE, parsed_messages)

    # ‚úÖ –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≤ –æ–¥–Ω–æ–º thread, —á—Ç–æ–±—ã –Ω–µ –ø–ª–æ–¥–∏—Ç—å –æ—à–∏–±–∫–∏/–≥–æ–Ω–∫–∏
    def _run_pipeline() -> None:
        run_build_parsed_etalon()
        run_build_parsed_goods()

        try:
            from handlers.parsing import results as results_mod
            run_results = getattr(results_mod, "run_results", None)
        except Exception:
            run_results = None

        if callable(run_results):
            run_results()

    await asyncio.to_thread(_run_pipeline)

    # –∫–∞—Å—Ç–æ–º: –¥–æ–ø–æ–ª–Ω—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π matched –Ω–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º parsed_data.json
    if (u or {}).get("role") != "admin" and (u or {}).get("sources_mode") == "custom":
        try:
            from handlers.parsing import results as results_mod
            base_matched = (DEFAULT_BASE_DIR / "parsed_matched.json")
            user_dir = user_data_dir(callback.from_user.id)
            user_matched = user_dir / "parsed_matched.json"

            def _load_items(p: Path) -> list[dict]:
                if not p.exists():
                    return []
                try:
                    raw = json.loads(p.read_text(encoding="utf-8"))
                except Exception:
                    return []
                if isinstance(raw, dict) and isinstance(raw.get("items"), list):
                    return [x for x in raw["items"] if isinstance(x, dict)]
                if isinstance(raw, list):
                    return [x for x in raw if isinstance(x, dict)]
                return []

            merged_items = _load_items(base_matched) + _load_items(user_matched)
            user_matched.write_text(
                json.dumps(
                    {"items": merged_items, "items_count": len(merged_items)},
                    ensure_ascii=False,
                    indent=2,
                ),
                encoding="utf-8",
            )
            set_parsing_data_dir(user_dir)
            results_mod.rebuild_parsed_data_all()
        except Exception:
            pass

    total_msgs = len(parsed_messages)
    total_lines = sum(int(m.get("lines_count") or 0) for m in parsed_messages)

    if callback.message:
        await callback.message.answer(
            f"‚úÖ –ì–æ—Ç–æ–≤–æ.\n"
            f"–°–æ–æ–±—â–µ–Ω–∏–π: <b>{total_msgs}</b>\n"
            f"–°—Ç—Ä–æ–∫: <b>{total_lines}</b>\n"
            f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: <b>{int((stats_sources or {}).get('total', 0) or 0)}</b>\n"
            f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: <b>{int((stats_sources or {}).get('processed', 0) or 0)}</b>\n"
            f"–û—à–∏–±–æ–∫: <b>{int((stats_sources or {}).get('errors', 0) or 0)}</b>\n"
            f"{errors_block}"
            f"{zeros_block}",
            reply_markup=collect_menu_keyboard(),
        )

    try:
        await callback.answer()
    except Exception:
        pass



# =========================
# IO helpers
# =========================
def _utcnow_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def _write_json(path: Path, payload: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

def _reset_data_dir_files() -> None:
    """
    –û—á–∏—â–∞–µ–º –í–°–ï —Ñ–∞–π–ª—ã –≤ DATA_DIR, –Ω–æ –ù–ï —É–¥–∞–ª—è–µ–º –∏—Ö:
      - *.json -> –∑–∞–ø–∏—Å—ã–≤–∞–µ–º []
      - –æ—Å—Ç–∞–ª—å–Ω—ã–µ -> –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É ""
    –ü–∞–ø–∫–∏ –≤–Ω—É—Ç—Ä–∏ DATA_DIR –Ω–µ —Ç—Ä–æ–≥–∞–µ–º.
    """
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    for p in DATA_DIR.iterdir():
        if not p.is_file():
            continue

        try:
            if p.suffix.lower() == ".json":
                p.write_text("[]", encoding="utf-8")
            else:
                p.write_text("", encoding="utf-8")
        except Exception:
            # –Ω–µ –≤–∞–ª–∏–º –ø–∞—Ä—Å–µ—Ä, –µ—Å–ª–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å
            pass



# =========================
# Stage 2: message-level drops (–≥–ª—É—à–∏–º —Ü–µ–ª–∏–∫–æ–º)
# =========================

# ‚úÖ –î–µ—Ñ–µ–∫—Ç–Ω—ã–µ –±–ª–æ–∫–∏ "–æ–±–º–µ–Ω/–±—Ä–∞–∫", "—É—Ü–µ–Ω–∫–∞" –∏ —Ç.–ø. ‚Äî –µ—Å–ª–∏ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏)
RE_DROP_DEFECT_HEADER = re.compile(
    r"(?is)^\s*(?:‚ú®\s*)?(?:[\W_]{0,6}\s*)?"
    r"(–æ–±–º–µ–Ω\s*/\s*–±—Ä–∞–∫|–æ–±–º–µ–Ω–∫–∞|–±—Ä–∞–∫|—É—Ü–µ–Ω–∫\w*|–≤–∏—Ç—Ä–∏–Ω\w*|"
    r"—Ä–µ–º–æ–Ω—Ç\w*|—Å—Ü\b|service\s*center|refurb\w*|ref\b|used|–±/—É|–±\\—É)\b"
)

# ‚úÖ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ/—Å–µ—Ä–≤–∏—Å–Ω—ã–µ –∞–Ω–æ–Ω—Å—ã (–Ω–µ –ø—Ä–∞–π—Å)
RE_DROP_ANNOUNCE = re.compile(
    r"(?is)\b("
    r"–æ–±–Ω–æ–≤–∏–ª[–∞–∏]\s+–Ω–∞–ª–∏—á–∏–µ|–Ω–∞–ª–∏—á–∏–µ\s+—Ç–æ–≤–∞—Ä(–æ–≤)?\s+–Ω–∞|"
    r"—Å–¥–µ–ª–∞—Ç—å\s+–∑–∞–∫–∞–∑|–∑–∞–¥–∞—Ç—å\s+–≤–æ–ø—Ä–æ—Å|–¥–ª—è\s+–∑–∞–∫–∞–∑–∞\s+–ø–∏—à–∏—Ç–µ|"
    r"–ø—Ä–∞–π—Å\s+(?:–∑–∞–∫—Ä—ã—Ç|–æ–±–Ω–æ–≤–ª[–µ—ë]–Ω|–æ–±–Ω–æ–≤–∏–ª–∏)|—Ä–∞–±–æ—Ç–∞–µ–º\s+–ø–æ\s+–∑–∞–ø—Ä–æ—Å—É|"
    r"–∑–∞–∫–∞–∑\s+–≤\s+–¥–∏—Ä–µ–∫—Ç|–≤\s+–ª–∏—á–∫—É|–≤\s+–ª—Å|"
    r"@[\w\d_]{3,}"
    r")\b"
)

# ‚úÖ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ / —Ñ–æ—Ä–º–∞—Ç –∑–∞–∫–∞–∑–∞ (–≥–ª—É—à–∏–º –ø–æ—Å—Ç —Ü–µ–ª–∏–∫–æ–º) ‚Äî –†–ê–°–®–ò–†–ï–ù–û (–ª–æ–≤–∏–º "–≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ")
RE_DROP_INSTRUCTION = re.compile(
    r"(?is)\b("
    r"–≤\s+(?:—Å–ª–µ–¥—É—é—â\w*|—Ç–∞–∫\w*|—ç—Ç\w*|–¥–∞–Ω–Ω\w*|—Ç–∞–∫–æ–º|—ç—Ç–æ–º|–¥–∞–Ω–Ω–æ–º)\s+—Ñ–æ—Ä–º–∞—Ç\w*|"
    r"–≤\s*—Ñ–æ—Ä–º–∞—Ç–µ|—Ñ–æ—Ä–º–∞—Ç\s+(?:–∑–∞–∫–∞–∑–∞|–∑–∞–ø—Ä–æ—Å–∞)|"
    r"–ø—Ä–∏–º–µ—Ä\s+(?:–∑–∞–∫–∞–∑–∞|–∑–∞–ø—Ä–æ—Å–∞)|–æ–±—Ä–∞–∑–µ—Ü\s+(?:–∑–∞–∫–∞–∑–∞|–∑–∞–ø—Ä–æ—Å–∞)|—à–∞–±–ª–æ–Ω\s+(?:–∑–∞–∫–∞–∑–∞|–∑–∞–ø—Ä–æ—Å–∞)|"
    r"–∫–∞–∫\s+(?:–æ—Ñ–æ—Ä–º–∏—Ç—å|–Ω–∞–ø–∏—Å–∞—Ç—å|—Å–¥–µ–ª–∞—Ç—å)\s+(?:–∑–∞–∫–∞–∑|–∑–∞–ø—Ä–æ—Å)|"
    r"–∑–∞–∫–∞–∑—ã\s+–ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è|"
    r"–∑–∞–ø—Ä–æ—Å\s+–ø–æ\s+—Ç–æ–≤–∞—Ä—É\s+–¥–µ–ª–∞—Ç—å|–¥–µ–ª–∞—Ç—å\s+–∑–∞–ø—Ä–æ—Å|"
    r"–Ω–µ\s+–æ—Ç–ø–∏—Å—ã–≤–∞–µ–º|–Ω–µ\s+—É—Å–ø–µ–≤–∞–µ–º\s+–æ—Ç–ø–∏—Å—ã–≤–∞—Ç—å|—É–≤–∞–∂–∞–π—Ç–µ\s+–≤—Ä–µ–º—è|"
    r"–¥–ª—è\s+–∑–∞–∫–∞–∑–∞\s+–ø–∏—à–∏—Ç–µ|–∑–∞–∫–∞–∑\s+–≤\s+(?:–¥–∏—Ä–µ–∫—Ç|–ª–∏—á–∫—É|–ª—Å)|"
    r"–≥–∞—Ä–∞–Ω—Ç–∏–π–Ω—ã–π\s+—Å—Ä–æ–∫|–¥–æ\s+–∞–∫—Ç–∏–≤–∞—Ü–∏–∏|"
    r"–≤—ã–¥–∞—á–∞\s*/\s*–ø—Ä–∏–µ–º|–ø—Ä–æ–≤–µ—Ä–∫–∞\s+—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"
    r")\b"
)

# –†–æ–∑—ã–≥—Ä—ã—à–∏ / –±–∏–ª–µ—Ç—ã / –ø—Ä–æ–º–æ (–≥–ª—É—à–∏–º –ø–æ—Å—Ç —Ü–µ–ª–∏–∫–æ–º)
RE_DROP_GIVEAWAY = re.compile(
    r"(?is)\b("
    r"—Ä–æ–∑—ã–≥—Ä—ã—à|–∫–æ–Ω–∫—É—Ä—Å|"
    r"—Ä–∞–∑—ã–≥—Ä\w*|"              # ‚úÖ —Ä–∞–∑—ã–≥—Ä—ã–≤–∞—Ç—å/—Ä–∞–∑—ã–≥—Ä–∞–µ–º/—Ä–∞–∑—ã–≥—Ä—ã–≤–∞–µ–º/—Ä–∞–∑—ã–≥—Ä–∞–ª–∏
    r"give\s*away|giveaway|"
    r"–±–µ—Å–ø–ª–∞—Ç–Ω\w*\s+–±–∏–ª–µ—Ç|–±–∏–ª–µ—Ç(—ã)?\b|–ø—Ä–∏–∑(—ã)?\b|–ø—Ä–∏–∑–æ–≤(—ã—Ö)?\s+–º–µ—Å—Ç|"
    r"—É—á–∞—Å—Ç–≤(—É–π|—É–π—Ç–µ|–æ–≤–∞—Ç—å)\w*|—É—á–∞—Å—Ç–≤—É—é—Ç|"
    r"—É—Å–ø–µ–π(—Ç–µ)?\s+(?:–∫—É–ø–∏—Ç—å|–ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏)|"
    r"–≤—ã–ª–æ–∂(—É|–∏–º)\s+\d+\s+–±–∏–ª–µ—Ç|"
    r"—Ä–∞–∑–¥–µ–ª[–µ–∞]\s*\"?–±–∏–ª–µ—Ç(—ã)?\"?|@[\w\d_]{3,}|_bot\b|"
    r"–ø–æ–¥–∞—Ä(–æ–∫|–∫–∏)\b|üéÅ|üèÜ"
    r")\b"
)

# ‚Äú–®–∞–ø–∫–∏‚Äù –∏ –±–∞–Ω–Ω–µ—Ä—ã ‚Äî –Ω–µ –ø—Ä–∏—á–∏–Ω–∞ –≥–ª—É—à–∏—Ç—å, –Ω–æ –±—É–¥–µ–º —É–¥–∞–ª—è—Ç—å –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
RE_LINE_BANNER = re.compile(r"^(?:[\W_]{6,}|[=]{6,}|_{3,})$")


# =========================
# Stage 3: emoji cleanup (keep flags)
# =========================
# –û—Å—Ç–∞–≤–ª—è–µ–º üá∫üá∏üáØüáµ –∏ —Ç.–ø., —É–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω–æ–µ emoji/pictographs
RE_FLAGS = re.compile(r"[\U0001F1E6-\U0001F1FF]{2}")
RE_REMOVE_EMOJI_EXCEPT_FLAGS = re.compile(
    r"(?![\U0001F1E6-\U0001F1FF]{2})"
    r"[\U0001F000-\U0001FAFF\u2600-\u27BF]"
)


def strip_emoji_except_flags(text: str) -> str:
    if not text:
        return ""
    flags: List[str] = RE_FLAGS.findall(text)
    tmp = RE_FLAGS.sub("<<FLAG>>", text)

    tmp = RE_REMOVE_EMOJI_EXCEPT_FLAGS.sub("", tmp)

    for f in flags:
        tmp = tmp.replace("<<FLAG>>", f, 1)

    tmp = re.sub(r"[\u200d\uFE0F]", "", tmp)
    return tmp


# =========================
# Stage 4: line logic (YouTake-style join)
# =========================

# ‚úÖ –î–∞—Ç—ã/–≤—Ä–µ–º—è ‚Äî –≤—ã—Ä–µ–∑–∞–µ–º –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º —Ü–µ–Ω—ã, —á—Ç–æ–±—ã "2026" –Ω–µ —Å—á–∏—Ç–∞–ª–∞—Å—å —Ü–µ–Ω–æ–π
RE_DATE = re.compile(r"(?i)\b\d{1,2}[./-]\d{1,2}[./-]\d{2,4}\b")
RE_TIME = re.compile(r"(?i)\b\d{1,2}:\d{2}\b")
RE_YEAR = re.compile(r"(?i)\b20\d{2}\b")  # 2000-2099


def _strip_dates_times_for_price(s: str) -> str:
    s = RE_DATE.sub(" ", s or "")
    s = RE_TIME.sub(" ", s)
    # –µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –≥–æ–ª—ã–π –≥–æ–¥ ‚Äî —Ç–æ–∂–µ —É–±–∏—Ä–∞–µ–º (—á–∞—Å—Ç–æ –≤ –∞–Ω–æ–Ω—Å–∞—Ö)
    s = RE_YEAR.sub(" ", s)
    return _clean_spaces(s)


# =========================
# Price detection (supports "k-format": " - 19", " - 14,5", " - 2,3")
# =========================

# 1) –∫–ª–∞—Å—Å–∏–∫–∞: 45 900 / 45900 / 125000
_RE_PRICE_CLASSIC = r"(?:\d{1,3}(?:[ .]\d{3})+|\d{4,6})"

# 2) "—Ç—ã—Å—è—á–Ω—ã–π" —Ñ–æ—Ä–º–∞—Ç: —Å—Ç—Ä–æ–≥–æ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Ç–æ–≤–∞—Ä–∞:
#    –î–û–õ–ñ–ù–´ –±—ã—Ç—å –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ —Ç–∏—Ä–µ: " ... - 19", " ... ‚Äî 14,5"
#    (—Ç–∞–∫ –º—ã –ù–ï –ª–æ–≤–∏–º "10-20 –º–∏–Ω—É—Ç" –∏ –ù–ï –ª–æ–≤–∏–º ": 10")
_RE_PRICE_K_AFTER_DASH = r"(?:\s[-‚Äî]\s*\d{1,3}(?:[.,]\d{1,2})?)"

RE_PRICE = re.compile(rf"(?i)(?:{_RE_PRICE_CLASSIC}|{_RE_PRICE_K_AFTER_DASH})")

RE_PRICE_ONLY = re.compile(
    rf"(?i)^\s*(?:–æ—Ç\s*\d+\s*—à—Ç\s*[-‚Äî]?\s*)?"
    rf"(?:{_RE_PRICE_CLASSIC}|{_RE_PRICE_K_AFTER_DASH})"
    rf"\s*(?:‚ÇΩ|—Ä—É–±|—Ä\.|—Ä)?\s*$"
)

# ‚úÖ YouTakeBot: "–û—Ç N —à—Ç - PRICE" (—Å —Ñ–ª–∞–≥–æ–º/–±–µ–∑)
RE_YOUTAKE_TIER = re.compile(
    rf"(?i)^\s*(?:{RE_FLAGS.pattern}\s*)?"   # optional flag
    r"–æ—Ç\s*(\d+)\s*(?:—à—Ç\.?|—à—Ç—É–∫)\s*[-‚Äî]\s*"
    rf"({_RE_PRICE_CLASSIC})"
    r"\s*(?:‚ÇΩ|—Ä—É–±|—Ä\.|—Ä)?\s*$"
)


RE_YOUTAKE_TIER_ANY = re.compile(
    rf"(?i)(?:–æ—Ç\s*)?(\d+)\s*—à—Ç\s*[-‚Äî]\s*({_RE_PRICE_CLASSIC})"
)


# ‚Äú–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏‚Äù
RE_OOS = re.compile(r"(?i)\b(–Ω–µ—Ç\s*–≤\s*–Ω–∞–ª–∏—á–∏–∏|out\s*of\s*stock|sold\s*out|‚ùå)\b")

# ‚úÖ –¥–µ—Ñ–µ–∫—Ç–∫–∞/—É—Ü–µ–Ω–∫–∞/–∞–∫—Ç–∏–≤/–æ–±–º–µ–Ω/–±/—É ‚Äî –†–ê–°–®–ò–†–ò–õ–ò
RE_DEFECT_LINE = re.compile(
    r"(?i)\b("
    r"—É—Ü–µ–Ω–∫\w*|–¥–µ—Ñ–µ–∫—Ç\w*|—Å–∫–æ–ª\w*|—Ü–∞—Ä–∞–ø\w*|–ø–æ–º—è—Ç\w*|–ø–æ–¥–º—è—Ç\w*|–ø—Ä–∏–º—è—Ç\w*|–≤–º—è—Ç\w*|—Ç—Ä–µ—â–∏–Ω\w*|"
    r"–º—è—Ç\w*|"
    r"–±–∏—Ç(—ã–π)?|—Ä–∞–∑–±–∏—Ç\w*|–±–∏—Ç—ã–π\s*–ø–∏–∫—Å–µ–ª\w*|"
    r"—Ä–µ–º–æ–Ω—Ç\w*|—Å—Ü\b|service\s*center|"
    r"–≥–∞—Ä–∞–Ω—Ç–∏\w*\s+(?:–≤—ã—à–ª\w*|–Ω–µ—Ç)|"
    r"–∑–∞–º–µ–Ω\w*\s+(?:–ø–ª–∞—Ç\w*|–¥–∏—Å–ø–ª–µ\w*|—ç–∫—Ä–∞–Ω\w*|–º–∏–∫—Ä–æ—Ñ–æ–Ω\w*|–∫–∞–º–µ—Ä\w*|–∞–∫–∫—É–º\w*|"
    r"–¥–∏–Ω–∞–º–∏–∫\w*|–∫–æ—Ä–ø—É—Å\w*|—à–ª–µ–π—Ñ\w*|—Ä–∞–∑—ä[–µ—ë]–º\w*|usb)|"
    r"–æ–±–º–µ–Ω\w*|–æ–±–º–µ–Ω–∫–∞|swap|refurb\w*|ref\b|"
    r"–≤–∏—Ç—Ä–∏–Ω\w*|–¥–µ–º–æ|ex[- ]?demo|used|–±/—É|–±\\—É|"
    r"–Ω–µ\s*–∞–∫—Ç–∏–≤|–∞–∫—Ç–∏–≤\w*|active|–æ—Ç–∫—Ä\w*|open|–ø–ª–æ–º–±\w*|"
    r"—Ä–∞–∑–º–æ—Ç–∞–Ω\w*|–∫–æ–º–ø–ª–µ–∫—Ç\s*–Ω–µ–ø–æ–ª\w*"
    r")\b"
)

RE_WHOLESALE_QTY = re.compile(r"(?i)\b–æ—Ç\s*(?:10|20|30|50|100)\s*—à—Ç\b")

# ‚úÖ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à—Ç—É–∫ –≤ —Å—Ç—Ä–æ–∫–∞—Ö –ø—Ä–∞–π—Å–∞/–ø—Ä–∏–º–µ—Ä–∞: "- 2—à—Ç", "2 —à—Ç", "1pcs"
RE_QTY_IN_LINE = re.compile(r"(?i)(?:^|[\s])[-‚Äî]?\s*\d+\s*(?:—à—Ç|—à—Ç—É–∫|pcs)\b")

# ‚úÖ –±–ª–æ–∫ "–ü—Ä–∏–º–µ—Ä/–Ω–∞–ø—Ä–∏–º–µ—Ä"
# –õ–û–í–ò–ú "–Ω–∞–ø—Ä–∏–º–µ—Ä" –í–ù–£–¢–†–ò –°–¢–†–û–ö–ò (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏) ‚Äî —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –ø—Ä–∏–º–µ—Ä-—Ç–æ–≤–∞—Ä –Ω–∏–∂–µ
RE_EXAMPLE_START = re.compile(r"(?i)\b(–ø—Ä–∏–º–µ—Ä|–Ω–∞–ø—Ä–∏–º–µ—Ä)\b")

# ‚úÖ –ø—Ä–∏–∑–Ω–∞–∫–∏ "—ç—Ç–æ —É–∂–µ –Ω–µ –ø—Ä–∏–º–µ—Ä", –∑–∞–∫—Ä—ã–≤–∞–µ–º example_mode
RE_EXAMPLE_END_HINT = re.compile(
    r"(?i)^\s*(?:"
    r"–≤—ã–¥–∞—á–∞|–ø—Ä–∏–µ–º|–∫—ç—à|–≥–∞—Ä–∞–Ω—Ç–∏—è|–≤–æ–∑–≤—Ä–∞—Ç|–¥–æ—Å—Ç–∞–≤–∫–∞|—Å–∞–º–æ–≤—ã–≤–æ–∑|"
    r"–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ|–≤–∞–∂–Ω–∞—è\s+–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è|—Ü–µ–Ω—ã\s+–∏\s+–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ|"
    r"–∑–∞–∫–∞–∑\s+—Å—á–∏—Ç–∞–µ—Ç—Å—è|–¥–ª—è\s+—Å–≤—è–∑–∏|–∫–æ–Ω—Ç–∞–∫—Ç|–∞–¥—Ä–µ—Å|–≤—Ä–µ–º—è|"
    r"—Ä–∞–±–æ—Ç–∞–µ–º|–ø—Ä–∞–π—Å\b|–ø–æ\s+–ø—Ä–∞–π—Å—É|–Ω–∞–ª–∏—á–∏–µ|–æ–ø–ª–∞—Ç–∞"
    r")\b"
)

# ‚úÖ "10-20 –º–∏–Ω—É—Ç" –∏ –ø–æ–¥–æ–±–Ω–æ–µ ‚Äî –Ω–µ —Ü–µ–Ω–∞
RE_NOT_PRICE_TAIL = re.compile(r"(?i)\b(–º–∏–Ω—É—Ç|–º–∏–Ω|—á–∞—Å|—á–∞—Å–æ–≤|–¥–Ω(?:—è|–µ–π)?|%|–ø—Ä–æ—Ü–µ–Ω—Ç)\b")
RE_RANGE_PREFIX = re.compile(r"^\s*\d+\s*[-‚Äî]\s*\d+\b")  # "10-20", "5-30"

# ‚úÖ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ (–ª–æ–≥–∏—Å—Ç–∏–∫–∞/–æ–ø–ª–∞—Ç–∞/–≥–∞—Ä–∞–Ω—Ç–∏—è/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
RE_INFO_GUARD = re.compile(
    r"(?i)\b("
    r"—Å–∞–º–æ–≤—ã–≤–æ–∑|–¥–æ—Å—Ç–∞–≤–∫\w*|–∫—É—Ä—å–µ—Ä\w*|—Å–∫–ª–∞–¥\w*|–∑–∞–≥—Ä—É–∑–∫\w*|"
    r"–æ–ø–ª–∞—Ç\w*|–Ω–∞–ª–∏—á–Ω\w*|–∫—É–ø—é—Ä\w*|–Ω–æ–º–∏–Ω–∞–ª\w*|—Å–¥–∞—á\w*|–ø—Ä–æ—Ü–µ–Ω—Ç|%|"
    r"–≥–∞—Ä–∞–Ω—Ç–∏\w*|–≤–æ–∑–≤—Ä–∞—Ç\w*|–æ–±–º–µ–Ω\w*|–¥–∏–∞–≥–Ω–æ—Å—Ç\w*|—Å–µ—Ä–≤–∏—Å–Ω\w*\s*—Ü–µ–Ω—Ç|"
    r"–∫–∞–∫\s+–æ—Ñ–æ—Ä–º–∏—Ç—å|–∫–∞–∫\s+–∑–∞–∫–∞–∑–∞—Ç—å|–ø—Ä–∏–º–µ—Ä|–æ–±—Ä–∞–∑–µ—Ü|—Ñ–æ—Ä–º–∞—Ç\s+–∑–∞–∫–∞–∑–∞|—à–∞–±–ª–æ–Ω\s+–∑–∞–∫–∞–∑–∞|"
    r"–∑–∞–∫–∞–∑\s+–Ω–µ\s+—Å—á–∏—Ç–∞–µ—Ç—Å—è|–Ω–µ\s+—Å—á–∏—Ç–∞–µ—Ç—Å—è\s+–ø—Ä–∏–Ω—è—Ç|"
    r"–ø–∏—à–∏—Ç–µ\s+–∑–∞\s*\d+\s*–º–∏–Ω—É—Ç|–ø–æ–∂–∞–ª—É–π—Å—Ç–∞\s+–ø–∏—à–∏—Ç–µ|"
    r"–≥—Ä–∞—Ñ–∏–∫\s+—Ä–∞–±–æ—Ç—ã|—Ä–µ–∂–∏–º\s+—Ä–∞–±–æ—Ç—ã|"
    r"—Ä–∞–±–æ—Ç–∞–µ–º|–ø–æ\s+–ø—Ä–∞–π—Å—É|–ø—Ä–∞–π—Å\b"
    r")\b"
)

# ‚úÖ —É—Ç–æ—á–Ω—è—é—â–∏–π —Ö–∏–Ω—Ç: "–Ω–∞–ø—Ä–∏–º–µ—Ä" –∫–∞–∫ –≤–≤–æ–¥–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (–∞ –Ω–µ —Ç–æ–≤–∞—Ä–Ω–∞—è —Å—Ç—Ä–æ–∫–∞)
RE_EXAMPLE_INSTRUCTION_HINT = re.compile(
    r"(?i)\b("
    r"–≤\s+—Å–ª–µ–¥—É—é—â\w*\s+—Ñ–æ—Ä–º–∞—Ç\w*|–≤\s+—Ç–∞–∫–æ–º\s+—Ñ–æ—Ä–º–∞—Ç\w*|"
    r"—Ñ–æ—Ä–º–∞—Ç\w*\s+(?:–∑–∞–∫–∞–∑–∞|–∑–∞–ø—Ä–æ—Å–∞)|"
    r"–∫–∞–∫\s+(?:–æ—Ñ–æ—Ä–º–∏—Ç—å|—Å–¥–µ–ª–∞—Ç—å)\s+(?:–∑–∞–∫–∞–∑|–∑–∞–ø—Ä–æ—Å)|"
    r"–∑–∞–ø—Ä–æ—Å\s+–ø–æ\s+—Ç–æ–≤–∞—Ä—É|–¥–µ–ª–∞—Ç—å\s+–≤\s+—Å–ª–µ–¥—É—é—â\w*\s+—Ñ–æ—Ä–º–∞—Ç\w*"
    r")\b"
)

RE_ENUM_PREFIX = re.compile(r"(?i)^\s*\d{1,2}[.)]\s+")
RE_SPEC_LINE_START = re.compile(r"(?i)^\s*(\d{1,4}\s*(gb|tb)\b|\d{1,3}\s*/\s*\d{2,4}\s*(gb|tb)?\b)")
RE_SPEC_LINE_NUMBERED = re.compile(r"(?i)^\s*\d{1,2}\s+\d{1,2}\s*/\s*\d{2,4}\s*(gb|tb)?\b")
RE_PRODUCT_TOKENS = re.compile(
    r"(?i)\b(iphone|ipad|macbook|imac|airpods|watch|apple|galaxy|samsung|pixel|xiaomi|"
    r"poco|redmi|realme|honor|huawei|oneplus|oppo|vivo|tecno|infinix|ps5|playstation)\b"
)
RE_VARIANT_HEADER = re.compile(r"(?i)\b(air|pro|max|plus|mini|ultra|m\d)\b|\d{1,2}")


# =========================
# Parsing result structs
# =========================
@dataclass
class DeletedItem:
    text: str
    reason: str


def _clean_spaces(s: str) -> str:
    s = (s or "").replace("\xa0", " ")
    s = re.sub(r"[ \t]{2,}", " ", s)
    return s.strip()


def _basic_lines_from_message(text: str) -> List[str]:
    raw_lines = (text or "").splitlines()
    out: List[str] = []
    for ln in raw_lines:
        ln = strip_emoji_except_flags(ln)
        ln = _clean_spaces(ln)
        if not ln:
            continue
        out.append(ln)
    return out


def _is_header_or_separator_line(line: str) -> bool:
    if not line:
        return True
    if RE_LINE_BANNER.match(line):
        return True

    alnum = sum(ch.isalnum() for ch in line)
    if alnum == 0 and len(line) >= 4:
        return True

    if len(line) >= 8 and len(set(line.replace(" ", ""))) <= 3:
        return True

    return False


def _is_example_start_line(ln: str) -> bool:
    """
    –°—Ç–∞—Ä—Ç "–ø—Ä–∏–º–µ—Ä/–Ω–∞–ø—Ä–∏–º–µ—Ä" –±–ª–æ–∫–∞:
    - –µ—Å—Ç—å "–ø—Ä–∏–º–µ—Ä/–Ω–∞–ø—Ä–∏–º–µ—Ä"
    - –∏ —Å—Ç—Ä–æ–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é/–≤–≤–æ–¥–Ω—É—é (–∞ –Ω–µ —Ç–æ–≤–∞—Ä)
    """
    if not ln:
        return False

    if not RE_EXAMPLE_START.search(ln):
        return False

    # –µ—Å–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ —É–∂–µ –µ—Å—Ç—å —Ü–µ–Ω–∞ ‚Äî —ç—Ç–æ –ø–æ—á—Ç–∏ —Ç–æ—á–Ω–æ –ù–ï "—Å—Ç–∞—Ä—Ç –ø—Ä–∏–º–µ—Ä–∞"
    probe = _strip_dates_times_for_price(ln)
    if RE_PRICE.search(probe):
        return False

    # —è–≤–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ‚Äî –ø–æ—á—Ç–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ "–ø—Ä–∏–º–µ—Ä"
    if RE_EXAMPLE_INSTRUCTION_HINT.search(ln):
        return True

    # fallback: "–Ω–∞–ø—Ä–∏–º–µ—Ä," / "–Ω–∞–ø—Ä–∏–º–µ—Ä:" / "–Ω–∞–ø—Ä–∏–º–µ—Ä" –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∞—è –≤–≤–æ–¥–Ω–∞—è
    if re.search(r"(?i)\b–Ω–∞–ø—Ä–∏–º–µ—Ä\b\s*[:,-]?\s*$", ln.strip()):
        return True

    # –∏–Ω–∞—á–µ —Ç–æ–∂–µ —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ä—Ç–æ–º, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ —Ç–æ–≤–∞—Ä (–±–µ–∑ —Ü–µ–Ω—ã)
    return True


def _strip_second_price(line: str) -> str:
    """
    –ï—Å–ª–∏ –≤ —Å—Ç—Ä–æ–∫–µ –¥–≤–∞ —Ü–µ–Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "X - 17900 - 17850"),
    –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Ü–µ–Ω—É, —Ñ–ª–∞–≥–∏ –Ω–∞ –∫–æ–Ω—Ü–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º.
    """
    if not line:
        return line
    prices = list(RE_PRICE.finditer(line))
    if len(prices) < 2:
        return line

    # Ignore year-like tokens (e.g., 2024) and price-like fragments inside codes (e.g., MXN63).
    price_matches = []
    for m in prices:
        raw_token = m.group(0)
        token = raw_token.replace(" ", "").replace(".", "")
        if len(token) == 4 and token.isdigit() and 2000 <= int(token) <= 2099:
            continue
        # Skip matches that start inside an alphanumeric token or continue a digit sequence.
        if m.start() > 0 and line[m.start() - 1].isalnum():
            continue
        if m.end() < len(line) and line[m.end()].isdigit():
            continue
        # Skip memory-like pairs like "17 256" or "16 128".
        if " " in raw_token:
            parts = [p for p in raw_token.split() if p.isdigit()]
            if len(parts) == 2:
                a, b = parts
                if b in {"64", "128", "256", "512", "1024", "2048"}:
                    try:
                        if int(a) <= 30:
                            continue
                    except Exception:
                        pass
        price_matches.append(m)

    if len(price_matches) < 2:
        return line

    # Require a clear separator between prices (e.g., "- 17900 - 17850").
    between = line[price_matches[0].end():price_matches[1].start()]
    if "-" not in between and "‚Äî" not in between:
        return line

    flags = RE_FLAGS.findall(line)
    cut_pos = price_matches[1].start()
    head = line[:cut_pos].rstrip(" -‚Äì‚Äî")
    if flags:
        head = f"{head} {' '.join(flags)}"
    return _clean_spaces(head)


def _apply_header_context(lines: List[str]) -> List[str]:
    """
    –ü—Ä–µ—Ñ–∏–∫—Å—É–µ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "256GB Blue - 35500")
    –ø–æ—Å–ª–µ–¥–Ω–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–º ("iPad Air 11 M3 Wi-Fi"), —á—Ç–æ–±—ã entry.py —Å–º–æ–≥ —Å–º—ç—Ç—á–∏—Ç—å.
    """
    out: List[str] = []
    current_header = ""
    base_header = ""

    for ln in lines:
        ln_clean = strip_emoji_except_flags(ln)
        ln_clean = _clean_spaces(ln_clean)
        if not ln_clean:
            continue

        ln_clean = _strip_second_price(ln_clean)

        probe = _strip_dates_times_for_price(ln_clean)
        has_price = bool(RE_PRICE.search(probe))

        if not has_price:
            if re.search(r"[A-Za-z–ê-–Ø–∞-—è]", ln_clean):
                header = re.sub(r"[:\s-]+$", "", ln_clean).strip()
                if RE_PRODUCT_TOKENS.search(header):
                    base_header = header
                    current_header = header
                elif RE_VARIANT_HEADER.search(header) and base_header:
                    current_header = _clean_spaces(f"{base_header} {header}")
                else:
                    # –Ω–µ –º–µ–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª/—Ç–∏–ø
                    pass
            out.append(ln_clean)
            continue

        ln_check = _clean_spaces(RE_FLAGS.sub(" ", ln_clean))
        if current_header and (RE_SPEC_LINE_START.search(ln_check) or RE_SPEC_LINE_NUMBERED.search(ln_check)) and not RE_PRODUCT_TOKENS.search(ln_check):
            out.append(_clean_spaces(f"{current_header} {ln_clean}"))
            continue

        out.append(ln_clean)

    return out


# =========================
# Stage 1.5: channel-level dedupe by header (keep only latest)
# =========================

RE_MENTIONS = re.compile(r"(?i)@\w{3,}")
RE_NON_WORD_PUNCT = re.compile(r"[^\w\s\u0400-\u04FF]+", re.UNICODE)  # –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è (–æ—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/_)
RE_CONTACT_LINE = re.compile(r"(?i)\b(–¥–ª—è\s+—Å–≤—è–∑–∏|–∫–æ–Ω—Ç–∞–∫—Ç|–∫–æ–Ω—Ç–∞–∫—Ç—ã)\b")


def _normalize_header_line(s: str) -> str:
    s = (s or "").strip().lower()
    if not s:
        return ""

    # –≤—ã–∫–∏–¥—ã–≤–∞–µ–º @mentions
    s = RE_MENTIONS.sub("", s)

    s = re.sub(r"\s+", " ", s)

    # –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è ‚Äî –¥–µ–ª–∞–µ–º –ø—É—Å—Ç–æ–π, —á—Ç–æ–±—ã –Ω–µ –≤–ª–∏—è–ª–∞ –Ω–∞ fingerprint
    if RE_CONTACT_LINE.search(s):
        return ""

    # —É–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ —Ç–æ–∫–µ–Ω—ã (I, l, x –∏ —Ç.–ø.)
    for _ in range(2):
        s = re.sub(r"(?i)\b[a-z]\b", " ", s)
        s = re.sub(r"\s+", " ", s).strip()

    # —É–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é/–¥–µ–∫–æ—Ä (–æ—Å—Ç–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞/—Ü–∏—Ñ—Ä—ã)
    s = RE_NON_WORD_PUNCT.sub(" ", s)
    s = re.sub(r"\s+", " ", s).strip()

    return s


def _extract_header_fingerprint(raw_text: str) -> str:
    """
    "–®–∞–ø–∫–∞" = –ø–µ—Ä–≤—ã–µ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –î–û –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ —Å —Ü–µ–Ω–æ–π.
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ, —á—Ç–æ–±—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —à–∞–ø–∫–∏ —Å–æ–≤–ø–∞–¥–∞–ª–∏
    –¥–∞–∂–µ –ø—Ä–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä ' I '), @—é–∑–µ—Ä–∞—Ö –∏ —Ç.–ø.
    """
    lines = _basic_lines_from_message(raw_text or "")
    if not lines:
        return ""

    head_parts: List[str] = []
    max_lines = 14  # —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –≥–∞—Ä–∞–Ω—Ç–∏—é/—É—Å–ª–æ–≤–∏—è/–≤–∞–∂–Ω–æ

    for ln in lines:
        if _is_header_or_separator_line(ln):
            continue

        probe = _strip_dates_times_for_price(ln)
        has_text_signal = any(ch.isalpha() for ch in probe) or bool(
            re.search(r"(?i)\b(usb|type-?c|iphone|ipad|airpods|dyson|whoop|starlink|dji|pro|max|ultra|m\d)\b", probe)
        )

        # –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ —Å—Ç—Ä–æ–∫–∞ —Ç–æ–≤–∞—Ä–∞ —Å —Ü–µ–Ω–æ–π ‚Äî —à–∞–ø–∫—É –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º
        if has_text_signal and RE_PRICE.search(probe) and not RE_OOS.search(ln):
            break

        ln2 = _normalize_header_line(ln)
        if not ln2:
            continue

        head_parts.append(ln2)
        if len(head_parts) >= max_lines:
            break

    fp = " | ".join(head_parts)
    fp = re.sub(r"\s+", " ", fp).strip()
    return fp


def _message_sort_key(m: Dict[str, Any]) -> Tuple[int, int]:
    dt = m.get("date")
    ts = 0
    if isinstance(dt, str) and dt:
        try:
            ts = int(datetime.fromisoformat(dt.replace("Z", "+00:00")).timestamp())
        except Exception:
            ts = 0
    mid = int(m.get("message_id") or 0)
    return (ts, mid)


def dedupe_messages_by_header_keep_latest(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ channel:
      - –≥—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ fingerprint —à–∞–ø–∫–∏
      - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º–æ–µ –Ω–æ–≤–æ–µ (–ø–æ date/message_id)
    """
    by_channel: Dict[str, List[Dict[str, Any]]] = {}
    for m in messages:
        ch = (m.get("channel") or "").strip() or "Unknown"
        by_channel.setdefault(ch, []).append(m)

    out: List[Dict[str, Any]] = []
    for ch, items in by_channel.items():
        items_sorted = sorted(items, key=_message_sort_key)  # —Å—Ç–∞—Ä—ã–µ -> –Ω–æ–≤—ã–µ

        best_by_fp: Dict[str, Dict[str, Any]] = {}
        for mm in items_sorted:
            fp = _extract_header_fingerprint(mm.get("message") or "")
            if not fp:
                # –µ—Å–ª–∏ —à–∞–ø–∫–∞ –ø—É—Å—Ç–∞—è ‚Äî –Ω–µ —Å–ª–∏–≤–∞–µ–º –≤ –æ–¥–Ω—É –≥—Ä—É–ø–ø—É
                fp = f"__empty__:{mm.get('message_id')}:{mm.get('date')}"
            best_by_fp[fp] = mm  # –ø–µ—Ä–µ–∑–∞–ø–∏—à–µ—Ç—Å—è => –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —Å–∞–º–æ–µ –Ω–æ–≤–æ–µ

        out.extend(best_by_fp.values())

    return sorted(out, key=_message_sort_key)


def _looks_like_price_list(lines: List[str]) -> bool:
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –ø—Ä–∞–π—Å,
    –º—ã –ù–ï –≥–ª—É—à–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º –¥–∞–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å "–≥–∞—Ä–∞–Ω—Ç–∏—è/–æ–±–º–µ–Ω/–¥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏" –∏ —Ç.–ø.
    """
    if not lines:
        return False

    price_like = 0
    for ln in lines:
        if _is_header_or_separator_line(ln):
            continue
        if RE_OOS.search(ln):
            continue

        probe = _strip_dates_times_for_price(ln)

        # —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å "–ø–æ—Ö–æ–∂–∞ –Ω–∞ —Ç–æ–≤–∞—Ä": –ª–∏–±–æ –µ—Å—Ç—å –±—É–∫–≤—ã, –ª–∏–±–æ —Ç–∏–ø–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
        has_text_signal = any(ch.isalpha() for ch in probe) or bool(
            re.search(
                r"(?i)\b(usb|type-?c|iphone|ipad|airpods|dyson|whoop|starlink|dji|pro|max|ultra|m\d)\b",
                probe,
            )
        )

        if has_text_signal and RE_PRICE.search(probe):
            price_like += 1
            if price_like >= 3:  # ‚úÖ –ø–æ—Ä–æ–≥: 3 —Ç–æ–≤–∞—Ä–Ω—ã—Ö —Å—Ç—Ä–æ–∫–∏ —Å —Ü–µ–Ω–æ–π
                return True

    return False


# –†–æ–∑—ã–≥—Ä—ã—à–∏ / –±–∏–ª–µ—Ç—ã / –ø—Ä–æ–º–æ (–≥–ª—É—à–∏–º –ø–æ—Å—Ç —Ü–µ–ª–∏–∫–æ–º)
RE_DROP_GIVEAWAY = re.compile(
    r"(?is)\b("
    r"—Ä–æ–∑—ã–≥—Ä—ã—à|–∫–æ–Ω–∫—É—Ä—Å|"
    r"—Ä–∞–∑—ã–≥—Ä\w*|"              # ‚úÖ —Ä–∞–∑—ã–≥—Ä—ã–≤–∞—Ç—å/—Ä–∞–∑—ã–≥—Ä–∞–µ–º/—Ä–∞–∑—ã–≥—Ä—ã–≤–∞–µ–º/—Ä–∞–∑—ã–≥—Ä–∞–ª–∏
    r"give\s*away|giveaway|"
    r"–±–µ—Å–ø–ª–∞—Ç–Ω\w*\s+–±–∏–ª–µ—Ç|–±–∏–ª–µ—Ç(—ã)?\b|–ø—Ä–∏–∑(—ã)?\b|–ø—Ä–∏–∑–æ–≤(—ã—Ö)?\s+–º–µ—Å—Ç|"
    r"—É—á–∞—Å—Ç–≤(—É–π|—É–π—Ç–µ|–æ–≤–∞—Ç—å)\w*|—É—á–∞—Å—Ç–≤—É—é—Ç|"
    r"—É—Å–ø–µ–π(—Ç–µ)?\s+(?:–∫—É–ø–∏—Ç—å|–ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏)|"
    r"–≤—ã–ª–æ–∂(—É|–∏–º)\s+\d+\s+–±–∏–ª–µ—Ç|"
    r"—Ä–∞–∑–¥–µ–ª[–µ–∞]\s*\"?–±–∏–ª–µ—Ç(—ã)?\"?|@[\w\d_]{3,}|_bot\b|"
    r"–ø–æ–¥–∞—Ä(–æ–∫|–∫–∏)\b|üéÅ|üèÜ"
    r")\b"
)

def _should_drop_message_entirely(message_text: str) -> Optional[str]:
    t = message_text or ""
    raw_lines = _basic_lines_from_message(t)
    first_lines = raw_lines[:4]
    head = "\n".join(first_lines)

    defect_head = re.search(
        r"(?is)^\s*(?:[\W_]{0,10}\s*)?"
        r"(–æ–±–º–µ–Ω\s*(?:/|\\|\s+)?\s*–±—Ä–∞–∫|–æ–±–º–µ–Ω–∫–∞|–±—Ä–∞–∫|—É—Ü–µ–Ω–∫\w*|–≤–∏—Ç—Ä–∏–Ω\w*|"
        r"—Ä–µ–º–æ–Ω—Ç\w*|refurb\w*|ref\b|used|–±/—É|–±\\—É)\b",
        head,
    )
    if defect_head:
        return "defect_header_message"

    # ‚úÖ –†–û–ó–´–ì–†–´–®–ò/–ü–†–û–ú–û ‚Äî –í–°–ï–ì–î–ê –≥–ª—É—à–∏–º —Ü–µ–ª–∏–∫–æ–º
    if RE_DROP_GIVEAWAY.search(t):
        return "giveaway_message"

    looks_like_price = _looks_like_price_list(raw_lines)

    # ‚úÖ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ + –º–Ω–æ–≥–æ —Å—Ç—Ä–æ–∫ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º "—à—Ç" => –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø—Ä–∏–º–µ—Ä –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –∑–∞–∫–∞–∑–∞, –≥–ª—É—à–∏–º –í–°–ï–ì–î–ê
    if RE_DROP_INSTRUCTION.search(t):
        qty_lines = 0
        for ln in raw_lines:
            if _is_header_or_separator_line(ln):
                continue
            probe = _strip_dates_times_for_price(ln)
            if RE_PRICE.search(probe) and RE_QTY_IN_LINE.search(ln):
                qty_lines += 1
        if qty_lines >= 2:
            return "instruction_message"

    if RE_DROP_INSTRUCTION.search(t) and not looks_like_price:
        return "instruction_message"
    if RE_DROP_ANNOUNCE.search(t) and not looks_like_price:
        return "announce_message"

    if "\n" in t:
        defect_lines = 0
        price_lines = 0
        for ln in raw_lines:
            if _is_header_or_separator_line(ln):
                continue
            probe = _strip_dates_times_for_price(ln)
            if RE_DEFECT_LINE.search(ln):
                defect_lines += 1
            if RE_PRICE.search(probe) and (any(ch.isalpha() for ch in probe) or "-" in probe or "‚Äî" in probe):
                price_lines += 1

        if defect_lines >= 3 and price_lines >= 3 and defect_lines >= int(price_lines * 0.6):
            return "defect_multiline_message"

    return None

def _join_youtake_pairs(lines: List[str]) -> List[str]:
    """
    –°–∫–ª–µ–π–∫–∞ YouTakeBot:
      A: —Å—Ç—Ä–æ–∫–∞ —Ç–æ–≤–∞—Ä–∞ (–æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "‚Ä¢ ")
      B: "<flag> –û—Ç N —à—Ç - PRICE"
        - N == 1: —Å–∫–ª–µ–∏–≤–∞–µ–º –≤ "A - PRICE <flags>"
        - N  > 1: —Ü–µ–Ω—É –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º, —Å—Ç—Ä–æ–∫—É B –≤—ã–∫–∏–¥—ã–≤–∞–µ–º (wholesale tier)
    –í–∞–∂–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Å —Ç–∏—Ä–µ, —á—Ç–æ–±—ã downstream –ø–∞—Ä—Å–µ—Ä—ã –Ω–µ –ª–æ–º–∞–ª–∏—Å—å.
    """
    out: List[str] = []
    i = 0

    def _dedup_flags(s: str) -> str:
        fl = RE_FLAGS.findall(s or "")
        if not fl:
            return ""
        # –¥–µ–¥—É–ø —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
        uniq: List[str] = []
        for f in fl:
            if f not in uniq:
                uniq.append(f)
        return "".join(uniq)

    while i < len(lines):
        a = lines[i]
        b = lines[i + 1] if i + 1 < len(lines) else None

        a_clean = _clean_spaces(a)

        if b:
            b_clean = _clean_spaces(b)

            # ‚úÖ YouTake tier line
            m = RE_YOUTAKE_TIER.match(b_clean)
            if m:
                qty = int(m.group(1) or 0)
                price = (m.group(2) or "").strip()

                # —Ñ–ª–∞–≥–∏ –∏–∑ A –∏ B, –±–µ–∑ –¥—É–±–ª–µ–π
                flags = _dedup_flags(a_clean + " " + b_clean)

                if qty == 1:
                    # ‚úÖ –ö–õ–Æ–ß–ï–í–û: –¥–æ–±–∞–≤–ª—è–µ–º " - " —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                    merged = _clean_spaces(f"{a_clean} - {price} {flags}".strip())
                    out.append(merged)
                # qty > 1: –≤—ã–∫–∏–¥—ã–≤–∞–µ–º tier line —Ü–µ–ª–∏–∫–æ–º (–Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–∏–∫—É–¥–∞)

                i += 2
                continue

            # ‚úÖ —Å—Ç–∞—Ä—ã–π –∫–µ–π—Å: B = —á–∏—Å—Ç–∞—è —Ü–µ–Ω–∞ (–Ω–∞ –≤—Å—è–∫–∏–π, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è)
            if RE_PRICE_ONLY.match(b_clean):
                # —Ü–µ–Ω–∞ –∏–∑ B
                pm = RE_PRICE.search(_strip_dates_times_for_price(b_clean))
                price = pm.group(0) if pm else ""
                if price:
                    flags = _dedup_flags(a_clean + " " + b_clean)
                    merged = _clean_spaces(f"{a_clean} - {price} {flags}".strip())
                    out.append(merged)
                    i += 2
                    continue

        out.append(a_clean)
        i += 1

    return out


def _filter_lines(lines: List[str]) -> Tuple[List[str], List[DeletedItem], List[str]]:
    kept: List[str] = []
    deleted: List[DeletedItem] = []
    deleted_rows_legacy: List[str] = []

    example_mode = False

    prev_price_line = False
    for ln in lines:
        # ‚úÖ —Å—Ç–∞—Ä—Ç "–ø—Ä–∏–º–µ—Ä/–Ω–∞–ø—Ä–∏–º–µ—Ä" (–ª–æ–≤–∏–º –∏ "–Ω–∞–ø—Ä–∏–º–µ—Ä," –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Å—Ç—Ä–æ–∫–∏-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
        if _is_example_start_line(ln):
            example_mode = True
            deleted.append(DeletedItem(text=ln, reason="example_block"))
            deleted_rows_legacy.append(ln)
            continue

        # ‚úÖ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–∏–º–µ—Ä–∞ –¥—Ä–æ–ø–∞–µ–º –≤—Å—ë, –ø–æ–∫–∞ –Ω–µ —É–≤–∏–¥–∏–º —è–≤–Ω—ã–π –∫–æ–Ω–µ—Ü –±–ª–æ–∫–∞
        if example_mode:
            if _is_header_or_separator_line(ln) or RE_EXAMPLE_END_HINT.search(ln):
                example_mode = False  # —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É –¥–∞–ª—å—à–µ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –æ–±—ã—á–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏
            else:
                deleted.append(DeletedItem(text=ln, reason="example_block"))
                deleted_rows_legacy.append(ln)
                continue

        if _is_header_or_separator_line(ln):
            deleted.append(DeletedItem(text=ln, reason="header_or_separator"))
            deleted_rows_legacy.append(ln)
            continue

        if RE_OOS.search(ln):
            deleted.append(DeletedItem(text=ln, reason="out_of_stock"))
            deleted_rows_legacy.append(ln)
            continue

        if RE_DEFECT_LINE.search(ln):
            deleted.append(DeletedItem(text=ln, reason="defective_or_used"))
            deleted_rows_legacy.append(ln)
            continue

        # ‚úÖ –ª–æ–≥–∏—Å—Ç–∏–∫–∞/–æ–ø–ª–∞—Ç–∞/–≥–∞—Ä–∞–Ω—Ç–∏—è/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ‚Äî –¥—Ä–æ–ø–∞–µ–º –∫–∞–∫ –∏–Ω—Ñ–æ, –¥–∞–∂–µ –µ—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å —Ü–∏—Ñ—Ä—ã
        if RE_INFO_GUARD.search(ln):
            deleted.append(DeletedItem(text=ln, reason="info_line"))
            deleted_rows_legacy.append(ln)
            continue

        # ‚úÖ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å—Ç—Ä–∞—Ö–æ–≤–æ—á–Ω—ã–π –∫–µ–π—Å: "1. ... 10-20 –º–∏–Ω—É—Ç" / "2. ... 10-30 –º–∏–Ω—É—Ç"
        if RE_ENUM_PREFIX.search(ln) and (RE_RANGE_PREFIX.search(ln) or RE_NOT_PRICE_TAIL.search(ln)):
            deleted.append(DeletedItem(text=ln, reason="not_a_price_range"))
            deleted_rows_legacy.append(ln)
            continue

        # ‚úÖ –æ—Ç—Å–µ–∫–∞–µ–º "10-20 –º–∏–Ω—É—Ç", "5-30 –º–∏–Ω—É—Ç" –∏ –ø–æ–¥–æ–±–Ω–æ–µ
        if RE_RANGE_PREFIX.search(ln) and RE_NOT_PRICE_TAIL.search(ln):
            deleted.append(DeletedItem(text=ln, reason="not_a_price_range"))
            deleted_rows_legacy.append(ln)
            continue

        # ‚úÖ wholesale tiers ("–æ—Ç N —à—Ç ...") ‚Äî –¥—Ä–æ–ø–∞–µ–º
        if re.match(r"(?i)^\s*–æ—Ç\s+\d+\s*—à—Ç\b", ln) and RE_PRICE.search(_strip_dates_times_for_price(ln)):
            deleted.append(DeletedItem(text=ln, reason="wholesale_tier"))
            deleted_rows_legacy.append(ln)
            continue

        # ‚úÖ –¶–µ–Ω–∞ –∏—â–µ—Ç—Å—è –ø–æ —Å—Ç—Ä–æ–∫–µ –ë–ï–ó –¥–∞—Ç/–≤—Ä–µ–º–µ–Ω–∏/–≥–æ–¥–∞, —á—Ç–æ–±—ã –∞–Ω–æ–Ω—Å—ã –Ω–µ –ø—Ä–æ–ª–µ–∑–∞–ª–∏
        ln_price_probe = _strip_dates_times_for_price(ln)
        if not RE_PRICE.search(ln_price_probe):
            deleted.append(DeletedItem(text=ln, reason="no_price"))
            deleted_rows_legacy.append(ln)
            continue

        kept.append(ln)
        prev_price_line = True

    return kept, deleted, deleted_rows_legacy


# =========================
# Main parse (pipeline)
# =========================
def parse_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    messages: —Å–ø–∏—Å–æ–∫ —Å—ã—Ä—å—è –æ—Ç collect_messages(), –≥–¥–µ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã:
      {channel, message_id, date, message}
    """
    parsed: List[Dict[str, Any]] = []

    for m in messages:
        raw_text = (m.get("message") or "").strip()
        raw_lines = _basic_lines_from_message(raw_text)

        deleted: List[DeletedItem] = []
        deleted_rows_legacy: List[str] = []

        # —à–∞–≥ 2: –≥–ª—É—à–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º
        drop_reason = _should_drop_message_entirely(raw_text)
        if drop_reason:
            for ln in raw_lines:
                deleted.append(DeletedItem(text=ln, reason=drop_reason))
                deleted_rows_legacy.append(ln)

            parsed.append(
                {
                    "channel": m.get("channel"),
                    "message_id": m.get("message_id"),
                    "date": m.get("date"),
                    "message": raw_text,
                    "lines": [],
                    "deleted_rows": deleted_rows_legacy,
                    "deleted": [d.__dict__ for d in deleted],
                    "lines_count": 0,
                    "parsed_at": _utcnow_iso(),
                }
            )
            continue

        # —à–∞–≥ 4: —Å–∫–ª–µ–π–∫–∞ (YouTake-—Å—Ç–∏–ª—å)
        joined = _join_youtake_pairs(raw_lines)
        # —à–∞–≥ 4.1: –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, iPad ...)
        joined = _apply_header_context(joined)

        # —à–∞–≥ 5: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫
        kept, del_items, del_rows = _filter_lines(joined)
        deleted.extend(del_items)
        deleted_rows_legacy.extend(del_rows)

        # ‚úÖ RULE: –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä + —Ä—è–¥–æ–º –¥–µ—Ñ–µ–∫—Ç–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ ("–Ω–µ –∞–∫—Ç–∏–≤", "—Ü–∞—Ä–∞–ø–∏–Ω–∞" –∏ —Ç.–ø.) => —Å–Ω–µ—Å—Ç–∏ –≤—Å—ë —Å–æ–æ–±—â–µ–Ω–∏–µ
        if len(kept) == 1 and any(d.reason == "defective_or_used" for d in deleted):
            all_lines = _basic_lines_from_message(raw_text)
            deleted = [DeletedItem(text=ln, reason="defect_context_message") for ln in all_lines]
            deleted_rows_legacy = all_lines[:]
            kept = []

        parsed.append(
            {
                "channel": m.get("channel"),
                "message_id": m.get("message_id"),
                "date": m.get("date"),
                "message": raw_text,
                "lines": kept,
                "deleted_rows": deleted_rows_legacy,
                "deleted": [d.__dict__ for d in deleted],
                "lines_count": len(kept),
                "parsed_at": _utcnow_iso(),
            }
        )

    return parsed


# =========================
# Collect messages (FIXED: –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ)
# =========================

def _find_sources_json() -> Optional[Path]:
    """
    ‚úÖ –ö–ê–ö –ë–´–õ–û –í –†–ê–ë–û–ß–ï–ú –ö–û–î–ï:
    –ò—â–µ–º sources.json –≤ CWD (–æ—Ç–∫—É–¥–∞ –∑–∞–ø—É—â–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å).
    –ò–º–µ–Ω–Ω–æ —ç—Ç–æ –∏ –±—ã–ª–æ –∫–ª—é—á–µ–≤—ã–º, –ø–æ—á–µ–º—É —É —Ç–µ–±—è –≤—Å—ë —Ä–∞–±–æ—Ç–∞–ª–æ.
    """
    p = Path("sources.json")
    return p if p.exists() and p.is_file() else None


def _load_sources_from_file() -> Tuple[Dict[str, List[Dict[str, Any]]], Optional[Path]]:
    """
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º 2 —Ñ–æ—Ä–º–∞—Ç–∞:
    1) { "channels": [...], "bots": [...]  }  ‚úÖ —Ç–≤–æ–π —Å—Ç–∞—Ä—ã–π —Ä–∞–±–æ—á–∏–π
    2) { "items": [...] } –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ [ ... ]  (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π)
    """
    path = _find_sources_json()
    if not path:
        return {"channels": [], "bots": []}, None

    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {"channels": [], "bots": []}, path

    # 1) —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
    if isinstance(data, dict) and ("channels" in data or "bots" in data):
        ch = data.get("channels", []) or []
        bt = data.get("bots", []) or []
        return {
            "channels": [x for x in ch if isinstance(x, dict)],
            "bots": [x for x in bt if isinstance(x, dict)],
        }, path

    # 2) –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç items/list
    items: List[Dict[str, Any]] = []
    if isinstance(data, dict) and isinstance(data.get("items"), list):
        items = [x for x in data["items"] if isinstance(x, dict)]
    elif isinstance(data, list):
        items = [x for x in data if isinstance(x, dict)]
    else:
        return {"channels": [], "bots": []}, path

    channels: List[Dict[str, Any]] = []
    bots: List[Dict[str, Any]] = []
    for s in items:
        t = (s.get("type") or s.get("source_type") or "channel").strip().lower()
        if t == "bot":
            bots.append(s)
        else:
            channels.append(s)

    return {"channels": channels, "bots": bots}, path


def _filter_sources_for_user(
    sources_pack: Dict[str, List[Dict[str, Any]]],
    user_id: int | None,
    sources_mode: str,
) -> Dict[str, List[Dict[str, Any]]]:
    if user_id is None:
        return sources_pack

    def _is_own(item: dict) -> bool:
        return item.get("user_id") == user_id

    def _is_default(item: dict) -> bool:
        return item.get("user_id") is None

    channels = sources_pack.get("channels", []) or []
    bots = sources_pack.get("bots", []) or []

    if sources_mode == "default":
        return {
            "channels": [s for s in channels if _is_default(s)],
            "bots": [s for s in bots if _is_default(s)],
        }
    if sources_mode == "custom":
        return {
            "channels": [s for s in channels if _is_own(s)],
            "bots": [s for s in bots if _is_own(s)],
        }
    # "own"
    return {
        "channels": [s for s in channels if _is_own(s)],
        "bots": [s for s in bots if _is_own(s)],
    }


async def _clients_map(user_id: int | None = None, include_default: bool = True) -> Dict[str, Any]:
    """
    ‚úÖ –í —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ get_all_clients() –≤–æ–∑–≤—Ä–∞—â–∞–ª dict.
    –ù–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø–æ–¥–¥–µ—Ä–∂–∏–º –∏ list.
    """
    if user_id is None:
        cl = get_all_clients()
    else:
        cl = await get_clients_for_user(user_id, include_default=include_default)
    if isinstance(cl, dict):
        return cl

    out: Dict[str, Any] = {}
    if isinstance(cl, (list, tuple)):
        for i, c in enumerate(cl):
            out[str(i)] = c
    return out


def _pick_client_for_source(clients: Dict[str, Any], src: Dict[str, Any]) -> Optional[Any]:
    """
    ‚úÖ –ö–ê–ö –ë–´–õ–û:
    src["account"] –º–∞—Ç—á–∏—Ç—Å—è –ø–æ –∫–ª—é—á—É –≤ clients (–∏ "@account" —Ç–æ–∂–µ)
    –∏–Ω–∞—á–µ –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–ª–∏–µ–Ω—Ç.
    """
    if not clients:
        return None

    acc = (src.get("account") or "").strip()
    if acc:
        c = clients.get(acc) or clients.get(f"@{acc}")
        if c:
            return c

    return next(iter(clients.values()), None)


def _source_display_name(src: Dict[str, Any]) -> str:
    return (src.get("name") or src.get("title") or src.get("channel") or "").strip() or "Unknown"


def _source_entity_ref(src: Dict[str, Any]) -> Any:
    """
    ‚úÖ –ö–ê–ö –ë–´–õ–û –í –¢–í–û–Å–ú –†–ê–ë–û–ß–ï–ú:
    resolve_entity(client, channel_id)
    """
    for k in ("channel_id", "peer_id", "chat_id", "id", "username", "entity"):
        v = src.get(k)
        if v is None:
            continue
        if isinstance(v, int):
            return v
        if isinstance(v, str) and v.strip():
            return v.strip()
    return None


async def _run_bot_scenario(client: Any, entity: Any, src: Dict[str, Any]) -> None:
    """
    ‚úÖ –ö–ê–ö –ë–´–õ–û:
    –ï—Å–ª–∏ scenario –Ω–µ—Ç ‚Äî –±–æ—Ç –ù–ï –ø–∞—Ä—Å–∏–º (–∏–Ω–∞—á–µ —Ç—è–Ω–µ–º —Å—Ç–∞—Ä—É—é –∏—Å—Ç–æ—Ä–∏—é)
    """
    scenario = src.get("scenario") or []
    if not isinstance(scenario, list) or not scenario:
        trigger_text = (src.get("trigger_text") or src.get("command") or src.get("text") or "").strip()
        if trigger_text:
            await client.send_message(entity, trigger_text)
            await asyncio.sleep(float(src.get("delay_sec", 1.5) or 1.5))
        return

    step_delay = float(src.get("scenario_delay_sec", 1.2) or 1.2)
    for step in scenario:
        if not isinstance(step, dict):
            continue
        value = (step.get("value") or "").strip()
        if not value:
            continue
        await client.send_message(entity, value)
        await asyncio.sleep(step_delay)


async def _get_last_message_id(client: Any, entity: Any) -> int:
    try:
        from telethon import functions  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç

        history = await client(
            functions.messages.GetHistoryRequest(
                peer=entity,
                limit=1,
                offset_id=0,
                offset_date=None,
                add_offset=0,
                max_id=0,
                min_id=0,
                hash=0,
            )
        )
        msgs = getattr(history, "messages", None) or []
        if not msgs:
            return 0
        return int(getattr(msgs[0], "id", 0) or 0)
    except Exception:
        return 0


async def _collect_new_messages_after_id(
    client: Any,
    entity: Any,
    min_id: int,
    attempts: int = 6,
    sleep_sec: float = 1.2,
) -> List[Any]:
    collected: List[Any] = []
    best_max = int(min_id or 0)

    from telethon import functions  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç

    for _ in range(max(1, int(attempts))):
        try:
            history = await client(
                functions.messages.GetHistoryRequest(
                    peer=entity,
                    limit=200,
                    offset_id=0,
                    offset_date=None,
                    add_offset=0,
                    max_id=0,
                    min_id=best_max,
                    hash=0,
                )
            )
            msgs = getattr(history, "messages", None) or []
        except Exception:
            msgs = []

        new_msgs: List[Any] = []
        for mm in msgs:
            mid = int(getattr(mm, "id", 0) or 0)
            if mid > best_max:
                new_msgs.append(mm)

        if new_msgs:
            collected.extend(new_msgs)
            best_max = max(best_max, max(int(getattr(mm, "id", 0) or 0) for mm in new_msgs))
            break

        await asyncio.sleep(float(sleep_sec))

    return collected


async def collect_messages(
    user_id: int | None = None,
    sources_mode: str = "default",
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    ‚úÖ –ò–¢–û–ì: –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∫–∞–∫ –≤ —Ç–≤–æ—ë–º —Ä–∞–±–æ—á–µ–º —Ñ–∞–π–ª–µ:
    - sources.json —á–∏—Ç–∞–µ–º –∏–∑ CWD
    - —Ñ–æ—Ä–º–∞—Ç channels/bots –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º
    - get_all_clients dict –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º
    - –¥–ª—è bot: –ø–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ü–û–°–õ–ï scenario (baseline_id)
    """
    sources_pack, sources_path = _load_sources_from_file()
    sources_pack = _filter_sources_for_user(sources_pack, user_id, sources_mode)
    channels = sources_pack.get("channels", []) or []
    bots = sources_pack.get("bots", []) or []

    if not channels and not bots:
        return [], {
            "total": 0,
            "processed": 0,
            "messages": 0,
            "errors": 0,
            "reason": "sources.json not found or empty",
            "sources_path": str(sources_path) if sources_path else None,
        }

    clients = await _clients_map(user_id=user_id, include_default=(sources_mode == "default"))
    if not clients:
        return [], {
            "total": 0,
            "processed": 0,
            "messages": 0,
            "errors": 0,
            "reason": "no telethon clients",
            "sources_path": str(sources_path) if sources_path else None,
        }

    all_messages: List[Dict[str, Any]] = []

    stats: Dict[str, Any] = {
        "total": int(len(channels) + len(bots)),
        "processed": 0,
        "messages": 0,
        "errors": 0,
        "sources_path": str(sources_path) if sources_path else None,
        "per_source": [],
    }

    async def _collect_one(src: Dict[str, Any], source_type: str) -> None:
        nonlocal all_messages, stats

        title = _source_display_name(src)
        entity_ref = _source_entity_ref(src)
        if entity_ref is None:
            stats["errors"] += 1
            stats["per_source"].append({"source": title, "ok": False, "error": "no_entity_ref"})
            return

        client = _pick_client_for_source(clients, src)
        if not client:
            stats["errors"] += 1
            stats["per_source"].append({"source": title, "ok": False, "error": "no_client"})
            return

        try:
            entity = await resolve_entity(client, entity_ref)
        except Exception as e:
            stats["errors"] += 1
            stats["per_source"].append({"source": title, "ok": False, "error": f"resolve_entity: {e}"})
            return

        got = 0

        try:
            if source_type == "bot":
                scenario = src.get("scenario") or []
                if not isinstance(scenario, list) or not scenario:
                    stats["per_source"].append({"source": title, "ok": True, "messages": 0, "skipped": "no_scenario"})
                    stats["processed"] += 1
                    return

                baseline_id = await _get_last_message_id(client, entity)
                await _run_bot_scenario(client, entity, src)
                await asyncio.sleep(float(src.get("post_scenario_delay_sec", 1.6) or 1.6))

                msgs = await _collect_new_messages_after_id(
                    client,
                    entity,
                    min_id=baseline_id,
                    attempts=int(src.get("wait_attempts", 6) or 6),
                    sleep_sec=float(src.get("wait_sleep_sec", 1.2) or 1.2),
                )
            else:
                from telethon import functions  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç

                history = await client(
                    functions.messages.GetHistoryRequest(
                        peer=entity,
                        limit=int(src.get("limit", 200) or 200),
                        offset_id=0,
                        offset_date=None,
                        add_offset=0,
                        max_id=0,
                        min_id=0,
                        hash=0,
                    )
                )
                msgs = getattr(history, "messages", None) or []

            for msg in msgs:
                text = getattr(msg, "message", None) or getattr(msg, "caption", None) or ""
                if not str(text).strip():
                    continue

                all_messages.append(
                    {
                        "channel": title,
                        "source_type": source_type,
                        "message_id": int(getattr(msg, "id", 0) or 0),
                        "date": msg.date.isoformat() if getattr(msg, "date", None) else None,
                        "message": str(text).strip(),
                    }
                )
                got += 1

            stats["processed"] += 1
            stats["messages"] += got
            stats["per_source"].append({"source": title, "ok": True, "messages": got})

        except Exception as e:
            stats["errors"] += 1
            stats["per_source"].append({"source": title, "ok": False, "error": str(e)})

    # ‚úÖ —Å—Ç—Ä–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –±–µ–∑ gather (–∏ –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏)
    for src in channels:
        await _collect_one(src, "channel")
    for src in bots:
        await _collect_one(src, "bot")

    return all_messages, stats
